{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChipIR Fluence calculation\n",
    "\n",
    "We start by importing all of the required libraries and external functions.\n",
    "\n",
    "Then we select the appropriate input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time, timedelta\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from calculate_flux import get_fluency_flux\n",
    "from calculate_flux import read_count_file\n",
    "from calculate_flux import calculate_open_periods\n",
    "\n",
    "# Directory path\n",
    "directory = \"./\"\n",
    "\n",
    "# Distance file retrieved from measurements at the location\n",
    "distance_factor_file = \"distance_file.txt\"\n",
    "\n",
    "# Devices of interest\n",
    "allowed_devices = ['bruno1', 'bruno2']\n",
    "\n",
    "# File containing the shutter open/close information\n",
    "shutter_file = \"shutter.txt\"\n",
    "\n",
    "# Time window for each run\n",
    "time_unit_offset = timedelta(hours=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering log data\n",
    "\n",
    "Filter all of the ```countlog``` files. These are retrieved from the facility directly. \n",
    "\n",
    "This should filter malformed or broken lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countlog-2023-12-04.txt\n",
      "Ignoring line (malformed):Data From ChipIR\n",
      "\n",
      "countlog-2023-12-05.txt\n",
      "Ignoring line (malformed):Data From ChipIR\n",
      "\n",
      "countlog-2023-12-06.txt\n",
      "Ignoring line (malformed):Data From ChipIR\n",
      "\n",
      "countlog-2023-12-07.txt\n",
      "Ignoring line (malformed):Data From ChipIR\n",
      "\n",
      "countlog-2023-12-08.txt\n",
      "Ignoring line (malformed):Data From ChipIR\n",
      "\n",
      "countlog-2023-12-09.txt\n",
      "Ignoring line (malformed):Data From ChipIR\n",
      "\n",
      "countlog-2023-12-10.txt\n",
      "Ignoring line (malformed):Data From ChipIR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all countlog files in the directory\n",
    "countlog_files = [filename for filename in os.listdir(directory) if filename.startswith(\"countlog\")]\n",
    "\n",
    "# Sort the countlog files in chronological order\n",
    "countlog_files.sort(key=lambda x: datetime.strptime(re.search(r\"\\d{4}-\\d{2}-\\d{2}\", x).group(), \"%Y-%m-%d\"))\n",
    "\n",
    "# Initialize an empty dataframe\n",
    "neutron_count_df = pd.DataFrame()\n",
    "\n",
    "# Loop through the countlog files in chronological order\n",
    "for filename in countlog_files:\n",
    "    # Get the full file path\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    print(filename)\n",
    "    # Read the count file and append it to the dataframe\n",
    "    neutron_count = read_count_file(file_path)\n",
    "    neutron_count_df = pd.concat([neutron_count_df, pd.DataFrame(neutron_count)]).reset_index(drop=True)\n",
    "\n",
    "neutron_count_df = neutron_count_df.rename(columns={0: \"Timestamp\", 1: \"Fission Count\"})\n",
    "\n",
    "# Load all distances before continue\n",
    "distance_data = pd.read_csv(distance_factor_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove the logs relative to times where the boards were not in the table (distance file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the data that is not in the distance file for each device\n",
    "devices_dataframe = {}\n",
    "\n",
    "for device in distance_data[\"board\"].unique():\n",
    "    if device in allowed_devices:\n",
    "        distance_line = distance_data[distance_data[\"board\"] == device]\n",
    "        start_line = distance_line[\"start\"].iloc[0]\n",
    "        end_line = distance_line[\"end\"].iloc[0]\n",
    "\n",
    "        start_dt = pd.to_datetime(start_line, format='%d/%m/%Y %H:%M:%S')\n",
    "        end_dt = pd.to_datetime(end_line, format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "        filtered_df = neutron_count_df[\n",
    "            (neutron_count_df[\"Timestamp\"] >= start_dt) &\n",
    "            (neutron_count_df[\"Timestamp\"] <= end_dt)\n",
    "        ]\n",
    "        \n",
    "        devices_dataframe[device] = filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the fluence\n",
    "\n",
    "Calculate the fluence for each device in ```allowed_devices``` that we selected at the beggining.\n",
    "\n",
    "Store it in ```device_dict``` dictionary, the keys are ```allowed_devices```. The fields for each device are as follows:\n",
    "\n",
    "| Start Date | Flux | Time Beam Off | Fluence |\n",
    "|------------|------|---------------|---------|\n",
    "| 2023-12-05 21:00:00.403 | 0.0 | 353.895         | 0.0     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device_dict = {}\n",
    "\n",
    "for device in allowed_devices:\n",
    "    device_flunce_pd = pd.DataFrame(columns=['Start Date', 'Flux', 'Time Beam Off', 'Fluence'])\n",
    "    \n",
    "    df = devices_dataframe[device]\n",
    "\n",
    "    distance_line = distance_data[\n",
    "        (distance_data[\"board\"].str.contains(device))\n",
    "    ]\n",
    "    facility_factor = float(distance_line[\"facility_factor\"])\n",
    "    distance_attenuation = float(distance_line[\"Distance attenuation\"])\n",
    "\n",
    "    start_dt = df[\"Timestamp\"].iloc[0]\n",
    "\n",
    "    while start_dt <= df[\"Timestamp\"].iloc[-1]:\n",
    "        end_dt = start_dt + time_unit_offset\n",
    "        \n",
    "        if end_dt > df[\"Timestamp\"].iloc[-1]:\n",
    "            break\n",
    "\n",
    "        mistake_time = pd.to_datetime(\"2023-12-08 10:23:00\")\n",
    "        if start_dt <= mistake_time and end_dt >= mistake_time:\n",
    "            # print(\"aaaaaa\",start_dt, end_dt,mistake_time)\n",
    "            start_dt = start_dt + time_unit_offset\n",
    "            continue\n",
    "\n",
    "        neutron_count = df[(df[\"Timestamp\"] >= start_dt) & (df[\"Timestamp\"] <= end_dt)].values\n",
    "        if len(neutron_count) == 0:\n",
    "            start_dt = start_dt + time_unit_offset\n",
    "            continue\n",
    "        \n",
    "\n",
    "        flux, time_beam_off = get_fluency_flux(\n",
    "            start_dt=start_dt,\n",
    "            end_dt=end_dt,\n",
    "            neutron_count=neutron_count,\n",
    "            facility_factor=facility_factor,\n",
    "            distance_attenuation=distance_attenuation,\n",
    "        )\n",
    "        \n",
    "        fluence = flux * time_unit_offset.total_seconds()  \n",
    "        data = [\n",
    "            [start_dt, flux, time_beam_off, flux * time_unit_offset.total_seconds()]\n",
    "        ]\n",
    "        device_flunce_pd = pd.concat([device_flunce_pd, pd.DataFrame(data, columns=['Start Date', 'Flux', 'Time Beam Off', 'Fluence'])], ignore_index=True)\n",
    "        # print(start_dt, flux, time_beam_off, fluence)\n",
    "\n",
    "        start_dt = end_dt\n",
    "    \n",
    "    device_dict[device] = device_flunce_pd\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Fluence data\n",
    "\n",
    "The data in ```device_dict``` contains the fluence. Now we display overall statistics of beam and all devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutter open for a total time of 64.46666666666667 hours\n",
      "\n",
      "------------------------- bruno1 -------------------------\n",
      "\t3.72 hours of beam off out of 54.3 hours\n",
      "\tTotal Fluence = 8.99e+11\n",
      "\n",
      "------------------------- bruno2 -------------------------\n",
      "\t3.72 hours of beam off out of 54.3 hours\n",
      "\tTotal Fluence = 8.85e+11\n"
     ]
    }
   ],
   "source": [
    "def display_periods(periods):\n",
    "    \"\"\" Display the open periods \"\"\"\n",
    "    total_duration = timedelta()\n",
    "    for start, end in periods:\n",
    "        total_duration += end - start\n",
    "        \n",
    "    total_duration_hours = total_duration.total_seconds() / 3600\n",
    "    print(f\"Shutter open for a total time of {total_duration_hours} hours\")\n",
    "\n",
    "def largest_stretch(df):\n",
    "    # Find the largest continuous stretch of rows without discontinuity\n",
    "    max_stretch = 0\n",
    "    current_stretch = 0\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "\n",
    "    for i in range(len(df) - 1):\n",
    "        if df.index[i] + 1 == df.index[i + 1]:\n",
    "            current_stretch += 1\n",
    "        else:\n",
    "            if current_stretch > max_stretch:\n",
    "                max_stretch = current_stretch\n",
    "                start_index = i - current_stretch\n",
    "                end_index = i\n",
    "            current_stretch = 0\n",
    "\n",
    "    if current_stretch > max_stretch:\n",
    "        max_stretch = current_stretch\n",
    "        start_index = len(filtered_df) - current_stretch\n",
    "        end_index = len(filtered_df) - 1\n",
    "\n",
    "    return df.iloc[start_index:end_index + 1]\n",
    "\n",
    "periods = calculate_open_periods(shutter_file)\n",
    "display_periods(periods)\n",
    "\n",
    "for device in device_dict:\n",
    "    print(f\"\\n------------------------- {device} -------------------------\")\n",
    "    device_dict[device] = device_dict[device][device_dict[device][\"Flux\"] != 0]\n",
    "    total_time_beam_off = device_dict[device][\"Time Beam Off\"].sum()\n",
    "    num_time_windows = device_dict[device].shape[0] * time_unit_offset.total_seconds() / 3600\n",
    "    total_fluence = device_dict[device][\"Fluence\"].sum()\n",
    "    print(f\"\\t{round(total_time_beam_off/3600, 2)} hours of beam off out of {num_time_windows} hours\") # in hours\n",
    "    print(f\"\\tTotal Fluence = {total_fluence:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data\n",
    "\n",
    "Filter the data frame of each device and filter by start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = device_dict[allowed_devices[0]][device_dict[allowed_devices[0]]['Time Beam Off'] < 100]\n",
    " \n",
    "filtered_df = filtered_df[filtered_df['Start Date'].dt.day == 7]\n",
    "filtered_df = filtered_df[filtered_df['Start Date'].dt.hour >=10]\n",
    "filtered_df = filtered_df[filtered_df['Start Date'].dt.hour < 15]\n",
    "# filtered_df = filtered_df[filtered_df['Start Date'].dt.minute >= 0]\n",
    "# filtered_df = filtered_df[filtered_df['Start Date'].dt.minute <=58]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example configuration filter\n",
    "\n",
    "This will filter device 0 from ```allowed_devices``` according to the requirements of the experiment.\n",
    "\n",
    "This part will vary a lot and depends on what the experiment demands, so only a simple example is provided where we filter the fluence by start date and for windows (```time_unit_offset```) with less than 100 seconds of beam off time.\n",
    "\n",
    "Then the largest continuous stretch of data is selected. We use it to calculate the mean fluence in that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window : 2023-12-07 10:36:00.403000 -- 2023-12-07 13:06:00.403000\n",
      "Mean Fluence: 4.52e+10\n"
     ]
    }
   ],
   "source": [
    "filtered_df = device_dict[allowed_devices[0]][device_dict[allowed_devices[0]]['Time Beam Off'] < 100]\n",
    " \n",
    "filtered_df = filtered_df[filtered_df['Start Date'].dt.day == 7]\n",
    "filtered_df = filtered_df[filtered_df['Start Date'].dt.time > time(10,35)]\n",
    "filtered_df = filtered_df[filtered_df['Start Date'].dt.time < time(14,7)]\n",
    "\n",
    "filter_stretch = largest_stretch(filtered_df)\n",
    "\n",
    "# largest_stretch\n",
    "end_date = filter_stretch['Start Date'].iloc[-1] + timedelta(minutes=6)\n",
    "print(f\"Window : {filter_stretch['Start Date'].iloc[0]} -- {filter_stretch['Start Date'].iloc[-1] + timedelta(minutes=6)}\")\n",
    "mean_fluence = filter_stretch['Fluence'].sum()\n",
    "print(f\"Mean Fluence: {mean_fluence:.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_filter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
